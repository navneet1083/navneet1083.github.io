---
title: "About Me"
permalink: /about/
---
<!--
<sub>
 Passionate about discovering insights into complex systems from data. Results oriented, decisive leader in Big Data space that combines an entrepreneurial spirit with corporate-refined execution in Big Data strategy. Creativity to go beyond current tools to deliver the best solution to the problem, ability and comfort with working independently and making key decisions on projects.
</sub>
<br>
<sub>
 Strong working knowledge of data mining techniques, including Regression analysis, clustering, Neural Networks, SVM (support vector machines); also preferred is familiarity with recommendation systems such as Collaborative filtering, k-nearest Neighbors, association rules, market basket analysis, SVD (singular value decomposition), matrix factorization methods. Desire to analyze large data sets, find the truth in data, and develop efficient processes for data analysis. For data exploratory analysis using tools like R, Octave and for predicting or scoring modelling using Python. Naive experience in Python for building model; use NumPy for numerical analysis and SciPy for scientific computation while building model.
</sub>
<br>
<sub>
 Active participation in online competition like TopCoders, CodeForces and Kaggle. Actively participation in MOOC for many courses which includes Verified and non-verified. Eager to learn more as fast learner for new technologies and research oriented motive.
</sub>
<br>
<sub>
 Specialties: Hadoop, MapReduce, HIVE, HBase, search query optimisation using elastic search and solr, prediction analysis using machine learning techniques for supervised and unsupervised data-sets, data mining techniques using cleaning; preprocessing; feature extraction; statistical and applied mathematics modelling, processing structured and un-structed data-sets
</sub>
-->
<br>

## This is from `blog-updt` branch, to check the github-pages

I am Deep Learning Programmer specialising in training Deep Learning model on computer vision and also visualising neural network attentions to see how model is doing. It includes Grad-CAM, neurons activations, layers visualisation etc. I have trained several models on object detection (large as satellite-images and small as cashews nuts), gaze detection, facial attributes (like gender, emotion, age, pose on single model).

[//]: # (Passionate about discovering insights into complex systems from data. Results oriented, decisive leader in Big Data space that combines an entrepreneurial spirit with corporate-refined execution in Big Data strategy. Creativity to go beyond current tools to deliver the best solution to the problem, ability and comfort with working independently and making key decisions on projects.)

Now, I have been investing effective timings in fine-tuning LLMs. I have done couple of PoCs and hosted website for question-answering with BERT/GPT model and vector database (pincecone;redis). I have been fine-tuning model like FLAN-T5, GPT with PEFT using LoRA, Adapter techniques which reduces the parameters to be trained hence most weights remain unchanged. Also worked on BLEU and ROUGE score for model performance and evaluation. I have also worked on models hosted on hugging face (like Llama, GPT etc) and benchmarks like HELM and GLUE.

I have done research on temporal data for determining fight, vandalism, bullying etc. I have trained these models on (with modified) MobileNets, Squeezenets, FireNets etc. to run on embedded devices like RockPro, Nvidia Nano etc. Where device is not concern, i trained models on ResNet-101 also with little bit modification as related to features extractions.

#### Summary of my DL/ML proficiency skills are:
- PyTorch (training on clusters, optimising neural networks, Grad-CAM,  statistical modeling, 2D, 3D, convolution, depth convolution, SK-Net optimised, trained with depth information for Medical science (CT-scan, MRI etc))
- Python (Threading and multiprocessing; Flask; scikit-learn, bayesian modelling, matplotlib, ggplot, visualising techniques)
- Shell scripting (running jobs from kernel, shell scripting, AWK, ruby, well versed with regex for fast parsing while making training datasets, cronjob initiation)
- Data Analysis (extensive analysis on data before going for training, visualisation in all respects to check the data forming which normalisation, plotting, features extraction and normalising decision factor)
- Deployment (deployment of product using docker-image with pip requirements file, Jira, cluster Jupiter notebook management, kubernetes scalable.
- Optimization (quantisation, model running by hyper parameters, prunning network layers, int8,int16,FP16 conversion while training)
- Model testing (test benchmark on trained model to check how good is model, Normalise Mutual Information, Adjust Mutual Information, R-precision, ROC/AUC curve, Precision, Recall, specificity (confusion matrix), F1-score)
- Squeezing model if size and accuracy is a BIG concern to run on embedded devices.
